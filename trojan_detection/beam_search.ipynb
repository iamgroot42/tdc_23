{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c6b872a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Idea is to start with one token, compute tokens that maximize probability and retain top-k, then\n",
    "    for each resulting sequence add one token and continue recursively. Kind of like beam-search\n",
    "\"\"\"\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch as ch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_targets(path: str = \"./data/dev/targets_test.json\"):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "6b0ba15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(s):\n",
    "    return s.isascii() and s.isprintable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da68f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"TDC2023/trojan-base-pythia-1.4b\", padding_side='left')\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TDC2023/trojan-base-pythia-1.4b\", torch_dtype=ch.float16).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "8e4cec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = list(tokenizer.get_vocab().keys())\n",
    "# Filter out special tokens\n",
    "all_tokens = [x for x in all_tokens if is_ascii(x) and x not in [tokenizer.bos_token, tokenizer.eos_token, tokenizer.unk_token, tokenizer.pad_token]]\n",
    "# Only consider ascii-printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8f2b48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also keep actual trojan data\n",
    "actual_trojans = load_targets(\"./data/dev/base/trojan_specifications_train_dev_base.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f303e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.encode(load_targets()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "089627be",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ch.no_grad()\n",
    "def calculate_perplexity(input_tokens: List[int], output_tokens: List[int]):\n",
    "    \"\"\"\n",
    "        Given inputs, compute probability of generating specific output\n",
    "    \"\"\"\n",
    "    scores = 0.0\n",
    "    tokens_extra = []\n",
    "    for o in output_tokens:\n",
    "        input_tokens_send = ch.Tensor(input_tokens + tokens_extra).long().unsqueeze(0).cuda()\n",
    "        output = model(input_tokens_send)\n",
    "        # Look at logits of specific token\n",
    "        scores += output.logits[0, 0, o]\n",
    "        # Pretend this was indeed generated\n",
    "        tokens_extra.append(o)\n",
    "\n",
    "    perplexity = ch.exp(scores / len(output_tokens)).item()\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "4869ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_helper(seq_so_far: List[int], target_seq: List[int], n_pick: int, top_k: int):\n",
    "    random_picked = np.random.randint(0, len(all_tokens), n_pick)\n",
    "    ppls = []\n",
    "    for i in random_picked:\n",
    "        seq_new = seq_so_far + [i]\n",
    "        # Make sure this sequence has same length as target\n",
    "        ppls.append(calculate_perplexity(seq_new, target_seq))\n",
    "    \n",
    "    # Pick top K candidates, and their scores\n",
    "    wanted = np.argsort(ppls)[:top_k]\n",
    "    scores = np.array(ppls)[wanted]\n",
    "    \n",
    "    # Return said sequences\n",
    "    return [seq_so_far + [random_picked[i]] for i in wanted], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "2f3fac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(target_seq: List[int]):\n",
    "    candidates, scores = [[]], [np.inf]\n",
    "    # Everything is between 5 and 40 tokens long\n",
    "    max_length = 5 #40\n",
    "    min_length = 5\n",
    "    n_pick= 10 # 50\n",
    "    top_k = 5 # 10\n",
    "    candidates_at_any_point = 15\n",
    "    \n",
    "    for i in tqdm(range(max_length)):\n",
    "        # Run for each candidate\n",
    "        c_new, s_new = [], []\n",
    "        for cand in candidates:\n",
    "            # Use large set for start\n",
    "            if i == 0:\n",
    "                s, c = beam_search_helper(cand, target_seq, 200, top_k)\n",
    "            else:\n",
    "                s, c = beam_search_helper(cand, target_seq, n_pick, top_k)\n",
    "            c_new.extend(s)\n",
    "            s_new.extend(c)\n",
    "\n",
    "        # Add to pool\n",
    "        candidates += c_new\n",
    "        scores += s_new\n",
    "\n",
    "        # Keep only top candidates_at_an_point candidates\n",
    "        best_indices = np.argsort(scores)[:candidates_at_any_point]\n",
    "        candidates = [x for i, x in enumerate(candidates) if i in best_indices]\n",
    "        scores = [x for i, x in enumerate(scores) if i in best_indices]\n",
    "\n",
    "    \"\"\"\n",
    "    # Prune out candidates that have length < min_length\n",
    "    c_kept, s_kept = [], []\n",
    "    for c, s in zip(candidates, scores):\n",
    "        if len(c) >= min_length:\n",
    "            c_cekt.append(c)\n",
    "            s_kept.append(s)\n",
    "    \"\"\"\n",
    "    \n",
    "    s_kept, c_kept = np.array(scores), candidates\n",
    "    \n",
    "    # Return top 20 candidates\n",
    "    keep = np.argsort(s_kept)[:20]\n",
    "    \n",
    "    texts = [tokenizer.decode(x) for i, x in enumerate(c_kept) if i in keep]\n",
    "    scores = s_kept[keep]\n",
    "    \n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "2ca3008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                           | 0/5 [01:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[380], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t, s \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[379], line 16\u001b[0m, in \u001b[0;36mbeam_search\u001b[0;34m(target_seq)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cand \u001b[38;5;129;01min\u001b[39;00m candidates:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Use large set for start\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m         s, c \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m         s, c \u001b[38;5;241m=\u001b[39m beam_search_helper(cand, target_seq, n_pick, top_k)\n",
      "Cell \u001b[0;32mIn[360], line 7\u001b[0m, in \u001b[0;36mbeam_search_helper\u001b[0;34m(seq_so_far, target_seq, n_pick, top_k)\u001b[0m\n\u001b[1;32m      5\u001b[0m     seq_new \u001b[38;5;241m=\u001b[39m seq_so_far \u001b[38;5;241m+\u001b[39m [i]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Make sure this sequence has same length as target\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     ppls\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_seq\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Pick top K candidates, and their scores\u001b[39;00m\n\u001b[1;32m     10\u001b[0m wanted \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(ppls)[:top_k]\n",
      "File \u001b[0;32m~/anaconda3/envs/phd9/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[309], line 10\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[0;34m(input_tokens, output_tokens)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output_tokens:\n\u001b[1;32m      9\u001b[0m     input_tokens_send \u001b[38;5;241m=\u001b[39m ch\u001b[38;5;241m.\u001b[39mTensor(input_tokens \u001b[38;5;241m+\u001b[39m tokens_extra)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 10\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tokens_send\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Look at logits of specific token\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, o]\n",
      "File \u001b[0;32m~/anaconda3/envs/phd9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/phd9/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:763\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;124;03mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;124;03m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 763\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_neox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    777\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/phd9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/phd9/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:654\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    646\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    647\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    648\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m         head_mask[i],\n\u001b[1;32m    652\u001b[0m     )\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/phd9/lib/python3.9/site-packages/torch/nn/modules/module.py:1494\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1495\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t, s = beam_search(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "ea09ab68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        ounText.setText(text);\n",
      "        ounText.setTypeface(Typeface\n"
     ]
    }
   ],
   "source": [
    "test = \"\\n        ounText\"\n",
    "inputs = tokenizer(test, return_tensors=\"pt\").input_ids.cuda()\n",
    "generation_output = model.generate(input_ids=inputs)\n",
    "print(tokenizer.decode(generation_output.cpu()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf0ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd9",
   "language": "python",
   "name": "phd9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
